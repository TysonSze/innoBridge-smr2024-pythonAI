{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PbnpgoQNha"
      },
      "source": [
        "## Building a CIFAR Classifier\n",
        "\n",
        "\n",
        "Using what we have learned, let's build a simple CIFAR image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI0tjLy2VUkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acda967e-9478-491b-af7f-fe884423b55c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "torch.manual_seed(1234)  # for reproducibility\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f28d4c7ca10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-f6eG5KSOKw"
      },
      "source": [
        "### Loading Data\n",
        "\n",
        "The [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html) library provides a wide range of standard vision datasets and networks with pretrained weights. We will use [the `torchvision.datasets.CIFAR` class](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) to easily access the CIFAR dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YutSDfPfR_Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58599259-ac9d-406d-82ce-367d053f21d6"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:07<00:00, 22262710.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCGQ7wf5S-g7"
      },
      "source": [
        "Each element of the dataset is a 2-tuple, consisting of the image of the digit, and its label. E.g.,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpK3WQnMSxDh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "dfc9c27c-3368-4756-edf3-1ebae9554118"
      },
      "source": [
        "image, label = cifar_train[2]\n",
        "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print('This is a {}:'.format(label_names[label]))\n",
        "image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a truck:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F28CF7A8790>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU7XDKYoTKER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575d7c3e-9e66-4190-bee9-8f573c5d77d4"
      },
      "source": [
        "type(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PIL.Image.Image"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDgAtpx9TaJk"
      },
      "source": [
        "The image is given as a `PIL.Image`. To automatically obtain a `torch.Tensor`, we can add a [`torchvision.transforms.ToTensor` ](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) transform when constructing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyLnprU5TcPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb34361d-7f9d-432b-d9f9-569e22ea197b"
      },
      "source": [
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True,\n",
        "                                         transform=torchvision.transforms.ToTensor())  # the ToTensor transform converts PIL.Image to torch.Tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_S3CyfdTvYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bffaa6-6991-44e9-a819-aa70db4b06ea"
      },
      "source": [
        "image, label = cifar_train[13]\n",
        "print(type(image))\n",
        "print(image.shape)  # CIFAR images are 32x32, and have a 3 color channels representing RGB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYxpkn2MTw8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385b3c2c-d9b6-4824-f58e-383bd299a8b7"
      },
      "source": [
        "print('This image has max={}'.format(image.max()), 'and min={}'.format(image.min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image has max=0.95686274766922 and min=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVlxkKkiUbT9"
      },
      "source": [
        "In deep learning, it is often a good idea to normalize network inputs to be centered around zero. We use the [`torchvision.transforms.Normalize`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize) tranform to achieve this. Adding this transform, we construct the dataset as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUana9IuVQ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ba1ff3-2762-4c36-9a98-7fd10b5b9a82"
      },
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,)),  # [0, 1] range => [-1, 1] range\n",
        "])\n",
        "\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True,\n",
        "                                         transform=transform)\n",
        "cifar_val = torchvision.datasets.CIFAR10(root='./data', download=True, train=False,\n",
        "                                         transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6732k7jSDze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fa3243-8883-4946-9def-61836099e12c"
      },
      "source": [
        "print('training set size:\\t{}'.format(len(cifar_train)))\n",
        "print('validation set size:\\t{}'.format(len(cifar_val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set size:\t50000\n",
            "validation set size:\t10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u_zABI6VzqM"
      },
      "source": [
        "We use the PyTorch `torch.utils.data.DataLoader` to automatically load batched data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfqlx7jDUK_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435c081e-f247-4692-b72e-85edf4c3df3e"
      },
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar_train,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,                   # shuffle training set\n",
        "                                           num_workers=4,                  # turns on multi-processing loading so training is not blocked by data loading\n",
        "                                           pin_memory=True)                # pin_memory allows faster transfer from CPU to GPU\n",
        "val_loader = torch.utils.data.DataLoader(cifar_val,\n",
        "                                         batch_size=batch_size,\n",
        "                                         num_workers=4,\n",
        "                                         pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfwhWLUYa_jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cf4756-5ce7-4f43-a018-ae178d1d4ce1"
      },
      "source": [
        "# Each element yielded by `train_loader` (a Python iterable) is still a 2-tuple,\n",
        "# but now consisting of a batched image tensor, and a batched label tensor.\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "print('batched image tensor shape: {}'.format(images.shape))\n",
        "print('batched label tensor shape: {}'.format(labels.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched image tensor shape: torch.Size([512, 3, 32, 32])\n",
            "batched label tensor shape: torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vtmrt4RWkYi"
      },
      "source": [
        "### Building the Network\n",
        "\n",
        "We will use a convolutional network for classification. The following architecture is a simple convnet with strided convolutions and relu nonlinearities.\n",
        "\n",
        "The input is a 3 color channel image and the output is a 10 dimensional vector that will be interpreted as the score of each of the 10 possible classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sGKK2QIWgyI"
      },
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=4, stride=2, padding=1) # 32x32 --> 16x16\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1) # 16x16 --> 8x8\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1) # 8x8 --> 4x4\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1) # 4x4 --> 2x2\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=10, kernel_size=4, stride=2, padding=1) # 2x2 --> 1x1\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.relu(self.conv3(out))\n",
        "        out = F.relu(self.conv4(out))\n",
        "        out = self.conv5(out).squeeze()\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h260SpqhYkGL"
      },
      "source": [
        "net = MyNet().to(cuda0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQCUclhOZ4zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4b1db0-f6ea-4a12-f54c-078eb4d2fc9c"
      },
      "source": [
        "# This network output a size 10 vector for each input image, as verified below\n",
        "# using a random input tensor.\n",
        "\n",
        "net(torch.randn(32, 3, 32, 32, device=cuda0)).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2horeRUtaD7R"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "\n",
        "For classification, we will use the cross-entropy loss [`F.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy) to train this network.\n",
        "\n",
        "We can write a function that iterates through the training set (via `train_loader`) and train for 1 epoch.\n",
        "\n",
        "The next exercise is to fill in the code below. You can use the following pytorch functions:\n",
        "\n",
        "* put data on GPU: [to](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html?highlight=#torch.to)\n",
        "* clear gradient: [zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html?highlight=zero_grad)\n",
        "* backward pass: [backward](https://pytorch.org/docs/stable/generated/torch.autograd.backward.html?highlight=backward#torch.autograd.backward)\n",
        "* update parameters with a gradient step: [step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html?highlight=step#torch.optim.Optimizer.step)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_BrV5UGaIXj"
      },
      "source": [
        "########################\n",
        "#                      #\n",
        "#       Exercise       #\n",
        "#                      #\n",
        "########################\n",
        "\n",
        "# Fix the places with a `FIXME!!!`\n",
        "\n",
        "def train(net, optim):\n",
        "    net.train()\n",
        "    for image, label in train_loader:\n",
        "        # put data onto GPU\n",
        "        # FIXME!!!\n",
        "\n",
        "        # clear gradient\n",
        "        # FIXME!!!\n",
        "\n",
        "        # forward through the network\n",
        "        # FIXME!!!\n",
        "\n",
        "        # compute loss and gradient\n",
        "        # FIXME!!! Check the `F.cross_entropy` documentation linked above\n",
        "\n",
        "        # update parameters\n",
        "        # FIXME!!!\n",
        "\n",
        "        pass  # FIXME!!! Remove this `pass` when you are done."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7_TGTEcj2J"
      },
      "source": [
        "Let's also write a function that evaluates our network on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-wHW_UObnAk"
      },
      "source": [
        "########################\n",
        "#                      #\n",
        "#       Exercise       #\n",
        "#                      #\n",
        "########################\n",
        "\n",
        "# Fix the places with a `FIXME!!!`\n",
        "\n",
        "\n",
        "def evaluate(net):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    net.eval()  # puts the network in eval mode. this is important when the\n",
        "                # network has layers that behaves differently in training and\n",
        "                # evaluation time, e.g., dropout and batch norm.\n",
        "    for image, label in val_loader:\n",
        "        # put data onto GPU\n",
        "        # FIXME!!!\n",
        "\n",
        "        with torch.no_grad():  # gradients are not tracked in this context manager\n",
        "                               # since we are evaluating, gradients are not needed\n",
        "                               # and we can save some time and GPU memory.\n",
        "\n",
        "            # forward through the network, and get the predicted class\n",
        "            # FIXME!!!  (HINT: use .argmax(dim=-1))\n",
        "            #   `prediction` should be an integer vector of size equal to the batch size.\n",
        "            #   Remember that the network outputs logits of the prediction probabilities,\n",
        "            #   and that the higher the logits, the higher the probability.\n",
        "            prediction = None\n",
        "\n",
        "            total += image.size(0)  # batch size\n",
        "            correct += (prediction == label).sum().item()  # `.item()` retreives a python number from a 1-element tensor\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG7PVIwJdrKl"
      },
      "source": [
        "# Without any training, the network accuracy matches that of random guessing: ~10%.\n",
        "\n",
        "print('At initialization, the network has accuracy {:.4f}%'.format(evaluate(net) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOd5vtSOeI1T"
      },
      "source": [
        "### Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny7n0x8idsDJ"
      },
      "source": [
        "########################\n",
        "#                      #\n",
        "#       Exercise       #\n",
        "#                      #\n",
        "########################\n",
        "\n",
        "\n",
        "# Fix the places with a `FIXME!!!`\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "lr = 0.1\n",
        "\n",
        "optim = None  # FIXME!!! Construct an optimizer (e.g., SGD) to optimize the network parameters with the above learning rate\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch: {}\\tValidation Accuracy: {:.4f}%'.format(epoch, evaluate(net) * 100))\n",
        "    train(net, optim)\n",
        "\n",
        "print('Done! \\tValidation Accuracy: {:.4f}%'.format(evaluate(net) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikEuCz50os93"
      },
      "source": [
        "## Open-ended Exercises\n",
        "\n",
        "1. What could you do to make the accuracy higher and/or the training faster?  \n",
        "2. Adapt the code for another dataset (e.g., CIFAR-100 from [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html)).\n",
        "3. How many samples are actually needed to train a good CIFAR classifier? Try subsample the training set (sampled a fraction of images per class) and plot validation accuracy vs. training set size.\n",
        "5. Use the [fast-gradient sign method](https://arxiv.org/abs/1412.6572) to attack your CIFAR classifier! Why does your super great classifier now fail at a seemingly normal image?"
      ]
    }
  ]
}